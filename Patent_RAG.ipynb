{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "aa67ab3b-0cf6-459b-b6a5-44768aa5fb60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Work by:\n",
    "# Amit shomrat - 308032218\n",
    "# Leon Nizovtsov - 314801713"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00d84fe7-10a4-4f6a-af37-2ad31d494db0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import fitz  # PyMuPDF\n",
    "import json\n",
    "import re\n",
    "import easyocr\n",
    "import numpy as np\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "import cv2\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.http.models import Distance, VectorParams, PointStruct, Filter, FieldCondition, MatchValue, MatchAny\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import uuid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "aac365e7-73d4-48ce-a324-ff1561a5cdb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === STEP 1: CHUNKING ===\n",
    "\n",
    "def debug_print_chunking(text_added, image_added):\n",
    "    \"\"\"\n",
    "    Print a debug summary of the text and image chunks added.\n",
    "    \"\"\"\n",
    "    summary = []\n",
    "    if text_added:\n",
    "        summary.append(\"✅ text\")\n",
    "    if image_added:\n",
    "        summary.append(f\"✅ image\")\n",
    "    if not text_added and not image_added:\n",
    "        summary.append(\"⚪ skipped\")\n",
    "    print(f\"({', '.join(summary)})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "9634dd11-332e-412b-9c4b-be365c31d8a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_text_chunk(text_splitter, text_content, page_num, all_metadata, pdf_path):\n",
    "    \"\"\"\n",
    "    Add a text chunk to the metadata.\n",
    "    Args:\n",
    "        text_content (str): The text content to add\n",
    "        page_num (int): The page number of the chunk\n",
    "        all_metadata (dict): The metadata dictionary\n",
    "        pdf_path (str): The path to the PDF file\n",
    "\n",
    "    Returns:\n",
    "        bool: True if the text chunk was added successfully, False otherwise\n",
    "    \"\"\"\n",
    "    print(\"Converting text chunk\")\n",
    "\n",
    "    try:\n",
    "        if text_content.strip():\n",
    "            text_chunks = text_splitter.split_text(text_content)\n",
    "            for i, chunk in enumerate(text_chunks):\n",
    "                all_metadata[pdf_path][\"chunks\"].append({\n",
    "                    \"type\": \"text\",\n",
    "                    \"page\": page_num + 1,\n",
    "                    \"chunk_number\": i,\n",
    "                    \"content\": chunk.strip()\n",
    "                })\n",
    "    except Exception as e:\n",
    "        print(f\"Error converting text chunk: {e}\")\n",
    "        return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "4a9fa5e9-f00d-4570-b36d-f3d55b94c3cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sheet_descriptions(image_path, page_num, model=\"llava:7b\", max_chars=300):\n",
    "    \"\"\"\n",
    "    Convert an image to text using Llava via subprocess.\n",
    "    Args:\n",
    "        image_path (str): Path to the image file\n",
    "        page_num (int): The page number of the chunk\n",
    "        model (str): The model to use for text extraction (ignored in subprocess version)\n",
    "        max_chars (int): The maximum number of characters to return\n",
    "    \n",
    "    Returns:\n",
    "        dict: A dictionary containing the image description and metadata\n",
    "        None: If the image file is not found or the processing fails\n",
    "    \"\"\"\n",
    "    if not os.path.exists(image_path):\n",
    "        print(f\"❌ Image file not found: {image_path}\")\n",
    "        return None\n",
    "    prompt = (\n",
    "        f\"Image: {image_path}\\n\"\n",
    "        \"You are an OCR-style diagram transcriber.\\n\"\n",
    "        \"Analyze the image and output ONLY in this format:\\n\"\n",
    "        \"Type: [Flowchart / Directed Graph / UML Diagram]\\n\"\n",
    "        \"NODES:\\n\"\n",
    "        \"- <NodeID or order>: \\\"<Exact text inside node>\\\" (approx. number of text lines)\\n\"\n",
    "        \"EDGES:\\n\"\n",
    "        \"<Node1> -> <Node2>\\n\"\n",
    "        \"<Node2> -> <Node3>\\n\"\n",
    "        \"...\\n\"\n",
    "        \"ALL TEXT:\\n\"\n",
    "        f\"Copy verbatim ALL text seen anywhere in the image (limit {max_chars} characters).\\n\"\n",
    "        \"RULES:\\n\"\n",
    "        \"- Do NOT add introductions or explanations.\\n\"\n",
    "        \"- Do NOT infer or interpret meaning.\\n\"\n",
    "        \"- If a node has no visible label, assign an incremental ID (Box1, Box2, …).\\n\"\n",
    "        \"- Output plain text only, following the format above.\"\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        # Construct the llava command\n",
    "        cmd = [\"ollama\",\"run\",\"llava:7b\"]\n",
    "\n",
    "        # Run the command and capture output\n",
    "        process = subprocess.Popen(\n",
    "            cmd,\n",
    "            stdin=subprocess.PIPE,\n",
    "            stdout=subprocess.PIPE,\n",
    "            stderr=subprocess.PIPE,\n",
    "            text=True,\n",
    "            encoding=\"utf-8\",     # ← add this\n",
    "            errors=\"replace\"      # ← and this (never crash on odd bytes)\n",
    "        )\n",
    "\n",
    "        # Send prompt to stdin\n",
    "        stdout, stderr = process.communicate(input=prompt)\n",
    "\n",
    "        if process.returncode != 0:\n",
    "            print(f\"❌ Llava process failed: {stderr}\")\n",
    "            return None\n",
    "\n",
    "        text = stdout.strip()\n",
    "        if not text:\n",
    "            print(\"❌ No valid response received from Llava\")\n",
    "            return None\n",
    "\n",
    "        # safety trim to max_chars at word boundary\n",
    "        if len(text) > max_chars:\n",
    "            text = text[:max_chars].rsplit(\" \", 1)[0] + \"…\"\n",
    "\n",
    "        return {\n",
    "            \"type\": \"image_description\",\n",
    "            \"page\": page_num,\n",
    "            \"content\": text,\n",
    "            \"image_path\": image_path\n",
    "        }\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Failed to run Llava: {e}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "3abc00b0-7723-4329-9793-4e3b923a4450",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_image_chunk(page, page_num, all_metadata, output_dir, pdf_path):\n",
    "    \"\"\"\n",
    "    Add an image chunk to the metadata.\n",
    "    Args:\n",
    "        page (fitz.Page): The page to extract the image from\n",
    "        page_num (int): The page number of the chunk\n",
    "        all_metadata (dict): The metadata dictionary\n",
    "        output_dir (str): The output directory\n",
    "        pdf_path (str): The path to the PDF file\n",
    "\n",
    "    Returns:\n",
    "        bool: True if the image chunk was added successfully, False otherwise\n",
    "    \"\"\"\n",
    "    print(\"Converting image chunk\")\n",
    "    try:\n",
    "        pix = page.get_pixmap()\n",
    "        img_data = pix.tobytes(\"png\")\n",
    "        image_filename = f'{pdf_path.replace(\".pdf\", \"\")}_page_{page_num + 1}.png'\n",
    "        image_path = os.path.join(output_dir, image_filename)\n",
    "        with open(image_path, 'wb') as f:\n",
    "            f.write(img_data)\n",
    "        all_metadata[pdf_path]['chunks'].append(sheet_descriptions(image_path, page_num + 1))\n",
    "    except Exception as e:\n",
    "        print(f\"Error converting image chunk: {e}\")\n",
    "        return False\n",
    "\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "c307a3f8-12e6-48c9-a989-715523de6402",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ocr_text_extraction (page):\n",
    "    \"\"\"\n",
    "    Extract text from a page using OCR.\n",
    "    Args:\n",
    "        page (fitz.Page): The page to extract the text from\n",
    "    \n",
    "    Returns:\n",
    "        str: The extracted text\n",
    "    \"\"\"\n",
    "    # Page extraction pre - processing and cleaning:\n",
    "    pix = page.get_pixmap(dpi=300)\n",
    "    img = np.frombuffer(pix.samples, dtype=np.uint8).reshape(pix.height, pix.width, pix.n)\n",
    "    if pix.n == 4:\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGRA2BGR)\n",
    "    elif pix.n == 3:\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    img = cv2.GaussianBlur(img, (3, 3), 0)\n",
    "    img = cv2.resize(img, None, fx=2, fy=2, interpolation=cv2.INTER_CUBIC)\n",
    "    \n",
    "    if img.ndim == 3:\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)  # or COLOR_RGBA2GRAY if needed\n",
    "\n",
    "    _, img = cv2.threshold(img, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "    \n",
    "    # Text extraction:\n",
    "    reader = easyocr.Reader(['en'])\n",
    "    # Tries two angles: 0 and 90 and provide the most confident result. \n",
    "    # We need to find the best angle for the page. the rotation info should try the angles that it set to, \n",
    "    # but the results are not accurate as rotating the page through the page.set_rotation(90) of fitz.\n",
    "    ocr_results = reader.readtext(img, rotation_info=[0, 90])\n",
    "    ocr_text = \" \".join([result[1] for result in ocr_results])\n",
    "    return ocr_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "34f8e17a-7fce-46ab-b0f6-4fbfcc7b8481",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_and_images_from_patent(pdf_path, output_dir=\"extracted_images\"):\n",
    "    \"\"\"\n",
    "    Extract text and images from a patent PDF file.\n",
    "    \n",
    "    Args:\n",
    "        pdf_path (str): Path to the patent PDF file\n",
    "        output_dir (str): Directory to save extracted images\n",
    "        enable_ocr (bool): Whether to use OCR for pages with minimal text (SLOW!)\n",
    "    \n",
    "    Returns:\n",
    "        list: List of chunks with metadata in the format:\n",
    "              {\"type\": \"text\", \"page\": page_number, \"content\": text or image_path}\n",
    "              {\"type\": \"image\", \"page\": page_number, \"image_description\": image_description, \"content\": image_path}\n",
    "    \"\"\"\n",
    "    # Create output directory if it doesn't exist\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    doc = fitz.open(pdf_path)\n",
    "    total_pages = len(doc)\n",
    "    all_metadata = {pdf_path: {\n",
    "                      \"chunks\": []}}\n",
    "    \n",
    "    separators = [\n",
    "    \"\\n\\n\",  # First try to split on double newlines (paragraphs)\n",
    "    \"! \",    # Split on exclamation marks followed by space\n",
    "    \"? \",    # Split on question marks followed by space\n",
    "    \". \",    # Split on periods followed by space\n",
    "    \"\\n\",    # Then try single newlines\n",
    "    \" \",     # Then spaces\n",
    "    \"\"       # Finally, character by character if needed\n",
    "    ]\n",
    "\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=500,\n",
    "        chunk_overlap=100,\n",
    "        length_function=len,\n",
    "        separators=separators,\n",
    "        is_separator_regex=False\n",
    "    )\n",
    "\n",
    "    print(f\"Processing {total_pages} pages...\")\n",
    "    \n",
    "    for page_num in range(total_pages):\n",
    "        page = doc[page_num]\n",
    "        print(f\"📄 Processing page {page_num + 1}/{total_pages}...\", end=\" \")\n",
    "        \n",
    "        # Extract text from page pdf plain text\n",
    "        text_content = page.get_text()\n",
    "        if not text_content.strip():\n",
    "            text_content = ocr_text_extraction(page)\n",
    "\n",
    "        matches = re.findall(r'sheet\\s+.+?\\s+of\\s+.+?(?=[\\.\\n]|$)', text_content, flags=re.IGNORECASE)\n",
    "        image_added = False\n",
    "        text_added = False\n",
    "        if matches:\n",
    "            image_added = add_image_chunk(page, page_num, all_metadata, output_dir, pdf_path)\n",
    "        else:\n",
    "            text_added = add_text_chunk(text_splitter, text_content, page_num, all_metadata, pdf_path)\n",
    "        debug_print_chunking(text_added, image_added)\n",
    "\n",
    "    # Close the document\n",
    "    doc.close()\n",
    "    print(f\"Extraction complete! Found {len([c for c in all_metadata[pdf_path]['chunks'] if c['type'] == 'text'])} text chunks and {len([c for c in all_metadata[pdf_path]['chunks'] if c['type'] == 'image'])} images.\")\n",
    "    \n",
    "    return all_metadata\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "1f65537e-297f-490a-a869-c821749642c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_chunks_metadata(chunks, metadata_file=\"all_metadata.json\"):\n",
    "    \"\"\"\n",
    "    Save the chunks metadata to a JSON file.\n",
    "    If file exists, merge new data with existing data.\n",
    "    \n",
    "    Args:\n",
    "        chunks (dict): Dictionary with PDF path as key and chunk data as value\n",
    "        metadata_file (str): Path to save the metadata file\n",
    "    \"\"\"\n",
    "    # Load existing data if file exists\n",
    "    existing_data = {}\n",
    "    if os.path.exists(metadata_file):\n",
    "        try:\n",
    "            with open(metadata_file, 'r', encoding='utf-8') as f:\n",
    "                existing_data = json.load(f)\n",
    "            print(f\"Loaded existing data from {metadata_file}\")\n",
    "        except (json.JSONDecodeError, Exception) as e:\n",
    "            print(f\"Warning: Could not load existing data ({e}), starting fresh\")\n",
    "            existing_data = {}\n",
    "    \n",
    "    # Merge new chunks with existing data\n",
    "    existing_data.update(chunks)\n",
    "    \n",
    "    # Save combined data\n",
    "    with open(metadata_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(existing_data, f, indent=2, ensure_ascii=False)\n",
    "        \n",
    "    print(f\"Metadata saved to {metadata_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "2a614d89-f096-4f79-b25c-9ebcfeadd2e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_chunks_metadata(metadata_file=\"all_metadata.json\"):\n",
    "    \"\"\"\n",
    "    Load chunks metadata from JSON file.\n",
    "    \n",
    "    Args:\n",
    "        metadata_file (str): Path to the metadata file\n",
    "        \n",
    "    Returns:\n",
    "        list: List of chunk dictionaries\n",
    "    \"\"\"\n",
    "    if not os.path.exists(metadata_file):\n",
    "        # Create empty JSON file if it doesn't exist\n",
    "        print(f\"Creating new metadata file: {metadata_file}\")\n",
    "        with open(metadata_file, 'w', encoding='utf-8') as f:\n",
    "            json.dump({}, f, indent=2)\n",
    "        return []\n",
    "    \n",
    "    # Context manager form - file always closed even if error\n",
    "    with open(metadata_file, 'r', encoding='utf-8') as f:\n",
    "        chunks = json.load(f)\n",
    "    \n",
    "    print(f\"Loaded {len(chunks)} groups of chunks from {metadata_file}\")\n",
    "    return chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "c53c47e2-f70a-4310-9307-0cbd19c92ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === STEP 2: VECTOR STORE ===\n",
    "def create_vector_store(chunks, model_name=\"all-MiniLM-L6-v2\", collection_name=\"patent_chunks\"):\n",
    "    \"\"\"\n",
    "    Create vector store using SentenceTransformer and Qdrant.\n",
    "    \n",
    "    Args:\n",
    "        chunks (list): List of chunk dictionaries\n",
    "        model_name (str): SentenceTransformer model name \n",
    "                          (passed default \"all-MiniLM-L6-v2\" which is popular and balanced\n",
    "                          embedding vector size 384)\n",
    "        collection_name (str): Qdrant collection name\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (qdrant_client, sentence_transformer_model)\n",
    "    \"\"\"\n",
    "    print(f\"\\n=== Step 2: Creating Vector Store ===\")\n",
    "\n",
    "    # Initialize SentenceTransformer\n",
    "    print(f\"Loading SentenceTransformer model: {model_name}\")\n",
    "    model = SentenceTransformer(model_name)\n",
    "    \n",
    "    # Extract text content for encoding\n",
    "    texts = [chunk['content'] for chunk in chunks]\n",
    "    \n",
    "    # Create embeddings\n",
    "    print(\"Creating embeddings for all text and image chunks...\")\n",
    "    embeddings = model.encode(texts, show_progress_bar=True)\n",
    "    vector_size = embeddings.shape[1]\n",
    "    print(f\"Created embeddings: {embeddings.shape[0]} vectors of size {vector_size}\")\n",
    "    \n",
    "    # Initialize in-memory (RAM) Qdrant client\n",
    "    print(\"Setting up in-memory Qdrant vector database...\")\n",
    "    client = QdrantClient(\":memory:\")\n",
    "    \n",
    "    # Create collection:\n",
    "    # 1. vectors_config: size=vector_size, distance=Distance.COSINE\n",
    "    # 2. size: number of dimensions in the vector space\n",
    "    # 3. distance: distance metric used for similarity search\n",
    "    # 4. COSINE: cosine similarity\n",
    "    # 5. id: unique identifier for each point\n",
    "    # 6. vector: embedding vector\n",
    "    client.create_collection(\n",
    "        collection_name=collection_name,\n",
    "        vectors_config=VectorParams(size=vector_size, distance=Distance.COSINE),\n",
    "    )\n",
    "    print(f\"Created Qdrant collection: {collection_name}\")\n",
    "    \n",
    "    # Prepare points for insertion\n",
    "    points = []\n",
    "    # Each chunk and its corresponding embedding are zipped together\n",
    "    # and then enumerated to get the index and the chunk and embedding\n",
    "    # the index is used to create a unique ID for each point\n",
    "    # the chunk is used to create the payload\n",
    "    # the embedding is used to create the vector\n",
    "    for i, (chunk, embedding) in enumerate(zip(chunks, embeddings)):\n",
    "        point = PointStruct(\n",
    "            id=str(uuid.uuid4()),  # Unique ID for each point\n",
    "            vector=embedding.tolist(),  # Convert numpy array to list\n",
    "            payload={\n",
    "                \"type\": chunk[\"type\"],\n",
    "                \"page\": chunk[\"page\"],\n",
    "                \"content\": chunk[\"content\"],\n",
    "                \"chunk_index\": i\n",
    "            }\n",
    "        )\n",
    "        points.append(point)\n",
    "    \n",
    "    # Insert vectors into Qdrant\n",
    "    client.upsert(\n",
    "        collection_name=collection_name,\n",
    "        points=points\n",
    "    )\n",
    "    \n",
    "    print(f\"✅ Stored {len(points)} vectors in Qdrant collection\")\n",
    "    print(f\"✅ Vector store ready for semantic search!\")\n",
    "    \n",
    "    return client, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "bff55727-1d2f-4e9c-b4ec-ad8b38788b2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === STEP 3: QUESTION INPUT ===\n",
    "def load_questions(questions_file=\"questions.txt\"):\n",
    "    \"\"\"\n",
    "    Load questions from a text file.\n",
    "    \n",
    "    Args:\n",
    "        questions_file (str): Path to the questions file\n",
    "        \n",
    "    Returns:\n",
    "        list: List of question strings\n",
    "    \"\"\"\n",
    "    print(f\"\\n=== Step 3: Loading Questions ===\")\n",
    "    \n",
    "    # Check if questions file exists\n",
    "    if not os.path.exists(questions_file):\n",
    "        print(f\"❌ Error: Questions file '{questions_file}' not found!\")\n",
    "        return []\n",
    "    \n",
    "    # Load questions from file\n",
    "    inside_hidden = False\n",
    "    count_hidden = 0\n",
    "    questions = []\n",
    "    try:\n",
    "        with open(questions_file, 'r', encoding='utf-8') as f:\n",
    "            for line_num, line in enumerate(f, 1):\n",
    "                question = line.strip()\n",
    "                if question == \"[[HIDDEN]]\":\n",
    "                    inside_hidden = True\n",
    "                    count_hidden += 1\n",
    "                    continue\n",
    "                if question == \"[[/HIDDEN]]\":\n",
    "                    inside_hidden = False\n",
    "                    count_hidden += 1\n",
    "                    continue\n",
    "                if not inside_hidden:\n",
    "                    questions.append(question)\n",
    "                    print(f\"  Q{line_num - count_hidden}: {question}\")\n",
    "        \n",
    "        print(f\"✅ Loaded {len(questions)} questions from '{questions_file}'\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error reading questions file: {e}\")\n",
    "        return []\n",
    "    \n",
    "    if not questions:\n",
    "        print(\"⚠️  No questions found in file!\")\n",
    "        return []\n",
    "    \n",
    "    return questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "13ec5a69-2e5e-4c7c-90e1-3b51cbeda5dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === STEP 4: RAG PROMPT CONSTRUCTION ===\n",
    "def retrieve_relevant_chunks(question, client, model, collection_name=\"patent_chunks\", top_k=3):\n",
    "    \"\"\"\n",
    "    Retrieve top-k relevant text chunks for a question using vector similarity.\n",
    "    \n",
    "    Args:\n",
    "        question (str): The question to search for\n",
    "        client: Qdrant client\n",
    "        model: SentenceTransformer model\n",
    "        collection_name (str): Name of Qdrant collection\n",
    "        top_k (int): Number of chunks to retrieve\n",
    "        \n",
    "    Returns:\n",
    "        list: List of relevant chunks with metadata including embeddings of the form:\n",
    "                                {\n",
    "                                    'content': str,\n",
    "                                    'page': int,\n",
    "                                    'chunk_index': int,\n",
    "                                    'similarity': float,\n",
    "                                    'embedding': list\n",
    "                                }\n",
    "    \"\"\"\n",
    "    # Convert question to embedding\n",
    "    question_embedding = model.encode([question])\n",
    "\n",
    "    # Search for similar chunks in Qdrant (with vectors) - using query_points (newer API)\n",
    "    search_results = client.query_points(\n",
    "        collection_name=collection_name,\n",
    "        query=question_embedding[0].tolist(),\n",
    "        limit=top_k,\n",
    "        query_filter=Filter(\n",
    "            must=[\n",
    "                FieldCondition(key=\"type\", match=MatchValue(value=\"text\"))\n",
    "            ]\n",
    "        ),\n",
    "        with_vectors=True  # Include vectors in results\n",
    "    )\n",
    "    \n",
    "    # Extract chunks with similarity scores and embeddings\n",
    "    relevant_chunks = []\n",
    "    for result in search_results.points:\n",
    "        relevant_chunks.append({\n",
    "            'content': result.payload['content'],\n",
    "            'page': result.payload['page'],\n",
    "            'chunk_index': result.payload['chunk_index'],\n",
    "            'similarity': result.score,\n",
    "            'embedding': result.vector  # Include the stored embedding\n",
    "        })\n",
    "    \n",
    "    return relevant_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "7f26dcd4-db0b-419a-be02-cbb1ed711daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Instead of taking the greatest score, we have to take the greatest for the top chosen texts chunks (For each chosen text, take the top image).\n",
    "def top_similar_images(relevant_chunks, chunks, max_images=2, client=None, collection_name=\"patent_chunks\", max_threshold=0.4):\n",
    "    \"\"\"\n",
    "    Find up to 2 most relevant image chunks based on similarity to the relevant text chunks.\n",
    "    \n",
    "    Args:\n",
    "        relevant_chunks (list): Already retrieved relevant text chunks (with embeddings) of the form:\n",
    "                                {\n",
    "                                    'content': str,\n",
    "                                    'page': int,\n",
    "                                    'chunk_index': int,\n",
    "                                    'similarity': float,\n",
    "                                    'embedding': list\n",
    "                                }\n",
    "        chunks (list): All chunks (text and image)\n",
    "        max_images (int): Maximum number of images to return\n",
    "        client: Qdrant client\n",
    "        \n",
    "    Returns:\n",
    "        dict of top-k most relevant image chunks of the form:\n",
    "                                {\n",
    "                                    'page': int,\n",
    "                                    'content': str,\n",
    "                                    'chunk_index': int,\n",
    "                                    'similarity': float,\n",
    "                                    'embedding': list\n",
    "                                }\n",
    "    \"\"\"\n",
    "    \n",
    "    # Find image chunks, for candidate store only the page number then compare with client Qdrant.\n",
    "    candidate_images = [chunk for chunk in chunks if chunk['type'] == 'image_description']\n",
    "    \n",
    "    if not candidate_images:\n",
    "        return []\n",
    "    \n",
    "    # Use pre-computed embeddings from relevant chunks (no re-encoding!)\n",
    "    relevant_text_embeddings = [chunk['embedding'] for chunk in relevant_chunks]\n",
    "    \n",
    "\n",
    "    # take the embeddings that match the candidate_images form client qdrant - using query_points (newer API)\n",
    "    candidates_images_results = client.query_points(\n",
    "        collection_name=collection_name,\n",
    "        query=[0.0] * 384,  # Dummy vector (not used for filtering)\n",
    "        limit=1000,  # Large limit to get all matches\n",
    "        query_filter=Filter(\n",
    "            must=[\n",
    "                FieldCondition(\n",
    "                    key=\"page\",\n",
    "                    match=MatchAny(any=[img['page'] for img in candidate_images])\n",
    "                ),\n",
    "                FieldCondition(\n",
    "                    key=\"type\", \n",
    "                    match=MatchValue(value=\"image_description\")\n",
    "                )\n",
    "            ]\n",
    "        ), with_vectors=True )\n",
    "    \n",
    "    candidates_images_embeddings = []\n",
    "    for result in candidates_images_results.points:\n",
    "        candidates_images_embeddings.append({\n",
    "            'page': result.payload['page'],\n",
    "            'content': result.payload['content'],\n",
    "            'chunk_index': result.payload['chunk_index'],\n",
    "            'similarity': result.score,\n",
    "            'embedding': result.vector\n",
    "        })\n",
    "\n",
    "\n",
    "    # TODO: we need to modify this to take img from given similirity score threshold.\n",
    "    \n",
    "    # Calculate max similarity between each image and any relevant text chunk\n",
    "    similarities = []\n",
    "    for candidate_embedding in candidates_images_embeddings:\n",
    "        # Get similarity scores between this image and all relevant text chunks\n",
    "        img_similarities = cosine_similarity([candidate_embedding['embedding']], relevant_text_embeddings)[0]\n",
    "        # Take the maximum similarity (best match with any relevant text)\n",
    "        max_similarity = max(img_similarities)\n",
    "        similarities.append(max_similarity)\n",
    "    \n",
    "    # Add similarity scores to image data\n",
    "    for i, img in enumerate(candidate_images):\n",
    "        img['similarity'] = similarities[i]\n",
    "    \n",
    "    # Sort by similarity (highest first) and return top max_images\n",
    "    candidate_images.sort(key=lambda x: x['similarity'], reverse=True)\n",
    "    selected_images = []\n",
    "    for img in candidate_images[:max_images]:\n",
    "        if img['similarity'] >= max_threshold:\n",
    "            selected_images.append(img)\n",
    "        else:\n",
    "            break\n",
    "    if selected_images:\n",
    "        # Debug: Print similarity scores\n",
    "        print(f\"     Top {max_images} image similarity scores (vs pre-computed relevant text embeddings):\")\n",
    "        for i, img in enumerate(selected_images):\n",
    "            print(f\"       Image {i+1}: Page {img['page']}, Max Similarity = {img['similarity']:.3f}, Path = {img['image_path']}\")\n",
    "        return selected_images\n",
    "    else:\n",
    "        print(f\"     No images found with similarity score >= {max_threshold}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "01d9bb66-cda7-46a5-be3f-5f6a47c964e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_rag_prompt(question, question_index, relevant_chunks, selected_images_chunks, max_context_bytes=2000):\n",
    "    \"\"\"\n",
    "    Construct RAG prompt with question, context, and images.\n",
    "    \n",
    "    Args:\n",
    "        question (str): The question\n",
    "        relevant_chunks (list): Retrieved text chunks\n",
    "        selected_images (list): Paths to relevant images\n",
    "        max_context_bytes (int): Maximum context length in bytes\n",
    "        \n",
    "    Returns:\n",
    "        str1: Formatted prompt for Llava\n",
    "        str2: Formatted prompt for Llama\n",
    "    \"\"\"\n",
    "    # Sort relevant chunks by similarity (highest first)\n",
    "    relevant_chunks.sort(key=lambda x: x['similarity'], reverse=True)\n",
    "    # Add relevant chunks to the context, but not exceeding the max_context_bytes limit\n",
    "    context_parts = []\n",
    "    total_bytes = 0\n",
    "    for chunk in relevant_chunks:\n",
    "        chunk_text = chunk['content']\n",
    "        chunk_bytes = len(chunk_text.encode('utf-8'))\n",
    "        \n",
    "        # Check if adding this chunk would exceed limit\n",
    "        if total_bytes + chunk_bytes <= max_context_bytes:\n",
    "            context_parts.append(f\"[Page {chunk['page']}] {chunk_text}\")\n",
    "            total_bytes += chunk_bytes\n",
    "        else:\n",
    "            # Add partial chunk to reach exactly max_context_bytes\n",
    "            remaining_bytes = max_context_bytes - total_bytes\n",
    "            if remaining_bytes > 50:  # Only add if meaningful amount remaining\n",
    "                # encode for snniping the exact amount of bytes, then decode to utf-8 back.\n",
    "                partial_text = chunk_text.encode('utf-8')[:remaining_bytes].decode('utf-8', errors='ignore')\n",
    "                context_parts.append(f\"[Page {chunk['page']}] {partial_text}\")\n",
    "            break\n",
    "\n",
    "    question_bytes = len(question.encode('utf-8'))\n",
    "    if selected_images_chunks:\n",
    "        images_context = []\n",
    "        image_list = \"\"\n",
    "        for index, image_chunk in enumerate(selected_images_chunks):\n",
    "            image_list += f\"\\nImage {index+1}: {image_chunk['image_path']}\"\n",
    "            images_context.append(f\"\\nImage {index+1}-{image_chunk['content']}\")\n",
    "        images_list_bytes = len(\"\".join(image_list).encode('utf-8'))\n",
    "        images_context_bytes = len(\"\".join(images_context).encode('utf-8'))\n",
    "        \n",
    "        # Join the context parts with newlines.\n",
    "        text_context = \"\\n\".join(context_parts)\n",
    "\n",
    "        # Construct final prompt for Llava\n",
    "        prompt_llava = f\"\"\"Question {question_index} [bytes: {question_bytes}]:\\n{question}\\nText-Context [bytes: {max_context_bytes - images_list_bytes - question_bytes}]:\\n{text_context.encode('utf-8')[:max_context_bytes - images_list_bytes - question_bytes].decode('utf-8', errors='ignore')}\\nImages-Paths[bytes: {images_list_bytes}]: {image_list}\"\"\"\n",
    "\n",
    "        # Build context from relevant chunks for Llama:\n",
    "        images_context = \"\".join(images_context)\n",
    "        context_bytes_with_images = max_context_bytes - images_context_bytes - question_bytes\n",
    "        prompt_llama = f\"\"\"Question {question_index} [bytes: {question_bytes}]:\\n{question}\\nText-Context [bytes: {context_bytes_with_images}]:\\n{text_context.encode('utf-8')[:context_bytes_with_images].decode('utf-8', errors='ignore')}\\nImages-Context [bytes: {images_context_bytes}]:\\n{images_context}\"\"\"\n",
    "    else:\n",
    "        prompt_llava = prompt_llama = f\"\"\"Question {question_index} [bytes: {question_bytes}]:\\n{question}\\nText-Context [bytes: {total_bytes}]:\\n{context_parts[:max_context_bytes]}\"\"\"    \n",
    "    \n",
    "    return prompt_llava, prompt_llama\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "912259f2-cecb-44d8-8dec-06865cf0e496",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the models based on the question prompt.\n",
    "def process_questions_with_rag(questions, chunks, client, model):\n",
    "    \"\"\"\n",
    "    Process all questions using RAG pipeline. (retrieve relevant chunks, top similar images, construct rag prompt)\n",
    "    \n",
    "    Args:\n",
    "        questions (list): List of questions\n",
    "        chunks (list): All chunks (text and image)\n",
    "        client: Qdrant client \n",
    "        model: SentenceTransformer model\n",
    "        \n",
    "    Returns:\n",
    "        list: List of constructed prompts of the form:\n",
    "                                {\n",
    "                                    'question': str,\n",
    "                                    'llava_prompt': str,\n",
    "                                    'llama_prompt': str,\n",
    "                                    'relevant_pages': list,\n",
    "                                    'selected_images_chunks': list\n",
    "                                }\n",
    "    \"\"\"\n",
    "    print(f\"\\n=== Step 4: RAG Prompt Construction ===\")\n",
    "    print(f\"Processing {len(questions)} questions...\")\n",
    "    \n",
    "    # Clear prompt files at the start of each run\n",
    "    with open(\"prompt_llama.txt\", \"w\") as f:\n",
    "        f.write(\"\")  # Clear the file\n",
    "    with open(\"prompt_llava.txt\", \"w\") as f:\n",
    "        f.write(\"\")  # Clear the file\n",
    "    \n",
    "    prompts = []\n",
    "    \n",
    "    for i, question in enumerate(questions, 1):\n",
    "        print(f\"\\n🔍 Processing Question {i}/{len(questions)}: '{question[:50]}...'\")\n",
    "        \n",
    "        # 1. Retrieve top-k relevant text chunks\n",
    "        relevant_chunks = retrieve_relevant_chunks(question, client, model, top_k=3)\n",
    "        print(f\"   Retrieved {len(relevant_chunks)} relevant chunks\")\n",
    "        \n",
    "        # Show text similarity scores\n",
    "        for j, chunk in enumerate(relevant_chunks):\n",
    "            print(f\"     Chunk {j+1}: Page {chunk['page']}, Similarity = {chunk['similarity']:.3f}\")\n",
    "        \n",
    "        # 2. Find nearby images using similarity scoring with relevant text\n",
    "        relevant_pages = [chunk['page'] for chunk in relevant_chunks]\n",
    "        selected_images_chunks = top_similar_images(relevant_chunks,chunks, max_images=2, client=client)\n",
    "        \n",
    "        # 3. Construct prompt\n",
    "        llava_prompt, llama_prompt = construct_rag_prompt(question, i, relevant_chunks, selected_images_chunks)\n",
    "        prompts.append({\n",
    "            'question': question,\n",
    "            'llava_prompt': llava_prompt,\n",
    "            'llama_prompt': llama_prompt,\n",
    "            'relevant_pages': relevant_pages,\n",
    "            'selected_images_chunks': selected_images_chunks\n",
    "        })\n",
    "        \n",
    "    print(f\"\\n✅ Constructed {len(prompts)} RAG prompts\")\n",
    "    return prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "a6382ee1-2d16-43fb-a44d-3ee812aef14a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === STEP 5: ANSWER GENERATION ===\n",
    "def call_ollama_llama(prompt, model=\"llama3:latest\", max_chars=300):\n",
    "    \"\"\"\n",
    "    Call LLaMA via ollama for text-only questions.\n",
    "    \n",
    "    Args:\n",
    "        prompt (str): The input prompt\n",
    "        model (str): LLaMA model to use\n",
    "        max_chars (int): Maximum characters for the answer\n",
    "        \n",
    "    Returns:\n",
    "        str: Generated answer\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Prepare the command to call ollama\n",
    "        cmd = [\"ollama\", \"run\", model]\n",
    "        \n",
    "        # Add instruction to limit response length\n",
    "        full_prompt = f\"\"\"{prompt}\n",
    "\n",
    "Please provide a concise answer based ONLY on the provided context. Do not use external knowledge. Keep your answer under {max_chars} characters.\"\"\"\n",
    "        \n",
    "        # Use subprocess to call ollama\n",
    "        process = subprocess.Popen(\n",
    "            cmd,\n",
    "            stdin=subprocess.PIPE,\n",
    "            stdout=subprocess.PIPE,\n",
    "            stderr=subprocess.PIPE,\n",
    "            text=True,\n",
    "            encoding='utf-8',\n",
    "            errors= \"replace\"\n",
    "        )\n",
    "        with open(\"prompt_llama.txt\", \"a\") as f:\n",
    "            f.write(full_prompt + \"\\n\\n\")\n",
    "        # Send prompt and get response\n",
    "        stdout, stderr = process.communicate(input=full_prompt, timeout=60)\n",
    "        \n",
    "        if process.returncode != 0:\n",
    "            print(f\"❌ Error calling ollama: {stderr}\")\n",
    "            return \"Error: Unable to generate answer\"\n",
    "        \n",
    "        # Clean and truncate the response\n",
    "        answer = stdout.strip()\n",
    "        if len(answer) > max_chars:\n",
    "            answer = answer[:max_chars].rsplit(' ', 1)[0] + \"...\"\n",
    "        \n",
    "        return answer\n",
    "        \n",
    "    except subprocess.TimeoutExpired:\n",
    "        print(\"❌ Timeout calling ollama\")\n",
    "        return \"Error: Timeout generating answer\"\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error calling ollama: {e}\")\n",
    "        return \"Error: Unable to generate answer\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "1f2c9c3f-c8eb-418b-a101-6ed903564c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_ollama_llava(prompt, model=\"llava:7b\", max_chars=300):\n",
    "    \"\"\"\n",
    "    Call LLaVA via ollama for text and image questions.\n",
    "    \n",
    "    Args:\n",
    "        prompt (str): The input prompt\n",
    "        model (str): LLaVA model to use\n",
    "        max_chars (int): Maximum characters for the answer\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        cmd = [\"ollama\", \"run\", model]\n",
    "        \n",
    "        full_prompt = f\"\"\"{prompt}\n",
    "Please provide a concise answer based ONLY on the provided context. Do not use external knowledge. Keep your answer under {max_chars} characters.\"\"\"\n",
    "        \n",
    "        process = subprocess.Popen(\n",
    "            cmd,\n",
    "            stdin=subprocess.PIPE,\n",
    "            stdout=subprocess.PIPE,\n",
    "            stderr=subprocess.PIPE,\n",
    "            text=True,\n",
    "            encoding='utf-8',\n",
    "            errors=\"replace\"\n",
    "        )\n",
    "        \n",
    "        with open(\"prompt_llava.txt\", \"a\") as f:\n",
    "            f.write(full_prompt + \"\\n\\n\")\n",
    "\n",
    "        stdout, stderr = process.communicate(input=full_prompt, timeout=120)\n",
    "        \n",
    "        if process.returncode != 0:\n",
    "            print(f\"❌ Error calling ollama llava: {stderr}\")\n",
    "            return \n",
    "        \n",
    "        answer = stdout.strip()\n",
    "        if len(answer) > max_chars:\n",
    "            answer = answer[:max_chars].rsplit(' ', 1)[0] + \"...\"\n",
    "        return answer\n",
    "    \n",
    "    except subprocess.TimeoutExpired:\n",
    "        print(\"❌ Timeout calling ollama llava\")\n",
    "        return \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error calling ollama llava: {e}\")\n",
    "        return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "a0d03c92-af48-46c4-a39d-109fb2c45310",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_ollama_models():\n",
    "    result = subprocess.run([\"ollama\", \"list\"], capture_output=True, text=True)\n",
    "    available = result.stdout.lower()\n",
    "\n",
    "    return {\n",
    "        \"llama3\": \"llama3\" in available,   # catches llama3:latest / llama3:8b / llama3:70b\n",
    "        \"llava\": \"llava\" in available\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "3ed0599e-9098-45d0-b7cf-60df37b42cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === STEP 6: ANSWERS TO FILE ===\n",
    "def generate_answers(rag_prompts, output_file=\"both_models_answers.txt\"):\n",
    "    \"\"\"\n",
    "    Generate answers for all questions using ollama (LLaMA/LLaVA).\n",
    "    \n",
    "    Args:\n",
    "        rag_prompts (list): List of RAG prompt dictionaries\n",
    "        output_file (str): File to save answers\n",
    "        \n",
    "    Returns:\n",
    "        list: List of answers\n",
    "    \"\"\"\n",
    "    print(f\"\\n=== Step 5: Answer Generation ===\")\n",
    "    print(f\"Generating answers for {len(rag_prompts)} questions using ollama...\")\n",
    "    \n",
    "    answers = []\n",
    "    \n",
    "    # Check if ollama is available and test models\n",
    "    try:\n",
    "        result = subprocess.run([\"ollama\", \"--version\"], \n",
    "                              capture_output=True, text=True, timeout=10)\n",
    "\n",
    "\n",
    "        if result.returncode != 0:\n",
    "            print(\"❌ Error: ollama is not available or not working properly\")\n",
    "            return []\n",
    "        \n",
    "        # Test available models\n",
    "        models = test_ollama_models()\n",
    "        \n",
    "        if not models.get('llama3'):\n",
    "            print(\"⚠️  Warning: No LLaMA model found. Run: ollama pull llama3\")\n",
    "        if not models['llava']:\n",
    "            print(\"⚠️  Warning: No LLaVA model found. You may need to run 'ollama pull llava'\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error: Cannot access ollama - {e}\")\n",
    "        print(\"   Please make sure ollama is installed and running\")\n",
    "        return []\n",
    "    \n",
    "    # Process each question\n",
    "    for i, prompt_data in enumerate(rag_prompts, 1):\n",
    "        question = prompt_data['question']\n",
    "        llava_prompt = prompt_data['llava_prompt']\n",
    "        llama_prompt = prompt_data['llama_prompt']\n",
    "        \n",
    "        # # Extract image paths safely (handle cases with 0, 1, or 2+ images)\n",
    "        # selected_images = prompt_data['selected_images_chunks']\n",
    "        # image_paths = [img['image_path'] for img in selected_images] if selected_images else []\n",
    "        \n",
    "        print(f\"\\n🤖 Generating answer {i}/{len(rag_prompts)}\")\n",
    "        print(f\"   Question: {question}\")\n",
    "        \n",
    "        # # ( We will use both Llama and Llava for image questions and Llama only for text questions (we have))\n",
    "        # if image_paths and len(image_paths) > 0:\n",
    "        #     print(f\"   Images include, using LLaVA with {len(image_paths)} images: {image_paths}\")\n",
    "\n",
    "        answer_llava = call_ollama_llava(llava_prompt)\n",
    "        answer_llava = answer_llava[:300]  # Ensure answers don't exceed 300 characters and handle None values\n",
    "        \n",
    "        answer_llama = call_ollama_llama(llama_prompt)\n",
    "        answer_llama = answer_llama[:300]\n",
    "        \n",
    "        # Handle None values for character counting\n",
    "        llama_chars = len(answer_llama) if answer_llama else 0\n",
    "        llava_chars = len(answer_llava) if answer_llava else 0\n",
    "        \n",
    "        answers.append({\n",
    "            'question_number': i,\n",
    "            'question': question,\n",
    "            'answer_llama': answer_llama or \"Error: LLaMA failed\",\n",
    "            'answer_llava': answer_llava or \"Error: LLaVA failed\", \n",
    "            'char_count_llama': llama_chars,\n",
    "            'char_count_llava': llava_chars\n",
    "        })\n",
    "        \n",
    "        print(f\"Answer LLaVA ({llava_chars} chars):\\n{answer_llava}\")\n",
    "        print(f\"Answer LLaMA ({llama_chars} chars):\\n{answer_llama}\")\n",
    "    \n",
    "    # Save answers to file\n",
    "    try:\n",
    "        with open(output_file, 'w', encoding='utf-8') as f:\n",
    "            for ans_data in answers:\n",
    "                f.write(f\"Question {ans_data['question_number']}: {ans_data['question']}\\n\")\n",
    "                f.write(f\"Answer LLaMA ({ans_data['char_count_llama']} chars):\\n{ans_data['answer_llama']}\\n\")\n",
    "                f.write(f\"Answer LLaVA ({ans_data['char_count_llava']} chars):\\n{ans_data['answer_llava']}\\n\")\n",
    "        \n",
    "        print(f\"\\n✅ Answers saved to {output_file}\")\n",
    "        \n",
    "        print(f\"   Total answers: {len(answers)}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error saving answers: {e}\")\n",
    "    \n",
    "    return answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "f043ac36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_similarity_results(evaluations, llama_avg, llava_avg, output_file=\"evaluation_results.txt\"):\n",
    "    \"\"\"\n",
    "    Save similarity evaluation results to a file.\n",
    "    \n",
    "    Args:\n",
    "        evaluations (dict): All evaluation data\n",
    "        llama_avg (float): LLaMA average similarity score\n",
    "        llava_avg (float): LLaVA average similarity score\n",
    "        output_file (str): Output file path\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(output_file, 'w', encoding='utf-8') as f:\n",
    "            f.write(\"=== SEMANTIC SIMILARITY EVALUATION RESULTS ===\\n\\n\")\n",
    "            \n",
    "            # Write detailed results for each question\n",
    "            for i, (question, data) in enumerate(evaluations.items(), 1):\n",
    "                f.write(f\"Question {i}: {data['question']}\\n\")\n",
    "                f.write(\"-\" * 80 + \"\\n\")\n",
    "                \n",
    "                # LLaMA evaluation\n",
    "                f.write(f\"LLaMA Answer: {data['llama_answer'][:100]}...\\n\")\n",
    "                f.write(f\"LLaMA Similarity Score: {data['llama_similarity']:.4f}\\n\\n\")\n",
    "                \n",
    "                # LLaVA evaluation\n",
    "                f.write(f\"LLaVA Answer: {data['llava_answer'][:100]}...\\n\")\n",
    "                f.write(f\"LLaVA Similarity Score: {data['llava_similarity']:.4f}\\n\")\n",
    "                \n",
    "                f.write(\"\\n\" + \"=\"*80 + \"\\n\\n\")\n",
    "            \n",
    "            # Write summary\n",
    "            f.write(\"=== SUMMARY ===\\n\\n\")\n",
    "            f.write(f\"LLaMA Average Similarity: {llama_avg:.4f}\\n\")\n",
    "            f.write(f\"LLaVA Average Similarity: {llava_avg:.4f}\\n\")\n",
    "            f.write(f\"Better Model: {'LLaVA' if llava_avg > llama_avg else 'LLaMA' if llama_avg > llava_avg else 'Tie'}\\n\")\n",
    "        \n",
    "        print(f\"✅ Detailed evaluation results saved to {output_file}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error saving evaluation results: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "75e14a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_single_answer(prompt, answer, model_name, collection_name=\"prompts_chunks\"):\n",
    "    \"\"\"\n",
    "    Evaluate a single answer based on semantic similarity with the prompt.\n",
    "    \n",
    "    Args:\n",
    "        prompt (str): The RAG prompt containing question and context\n",
    "        answer (str): The model's answer\n",
    "        model_name (str): SentenceTransformer model name\n",
    "        \n",
    "    Returns:\n",
    "        float: Semantic similarity score between prompt and answer (0-1 scale)\n",
    "    \"\"\"\n",
    "    # Handle error cases\n",
    "    if not answer or answer.startswith(\"Error:\"):\n",
    "        return 0.0\n",
    "    \n",
    "    # Initialize model\n",
    "    model = SentenceTransformer(model_name)\n",
    "    \n",
    "    # Encode entire prompt and answer as single embeddings\n",
    "    prompt_embedding = model.encode([prompt], show_progress_bar=False)\n",
    "    answer_embedding = model.encode([answer], show_progress_bar=False)\n",
    "    \n",
    "    # Compute cosine similarity between the two single embeddings\n",
    "    similarity_matrix = cosine_similarity(prompt_embedding, answer_embedding)\n",
    "    similarity_score = similarity_matrix[0][0]  # Extract the single similarity value\n",
    "    \n",
    "    return float(similarity_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "383cb267-14fe-4c3f-a391-55329c1ee3bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def answers_eval(rag_prompts, answers, output_file=\"answers.txt\", model_name=\"all-MiniLM-L6-v2\"):\n",
    "    \"\"\"\n",
    "    Evaluate answers from both LLaMA and LLaVA models using semantic similarity.\n",
    "    \n",
    "    This function computes the semantic similarity between each model's answer \n",
    "    and its corresponding RAG prompt (containing question + context).\n",
    "    \n",
    "    Args:\n",
    "        rag_prompts (list): List of RAG prompt dictionaries with 'llama_prompt' and 'llava_prompt'\n",
    "        answers (list): List of answer dictionaries with 'answer_llama' and 'answer_llava'\n",
    "        output_file (str): Path to save evaluation results\n",
    "        model_name (str): SentenceTransformer model for computing embeddings\n",
    "        \n",
    "    Returns:\n",
    "        dict: Evaluation results with similarity scores for each question and model\n",
    "    \"\"\"\n",
    "    print(f\"\\n=== Answer Evaluation ===\")\n",
    "    if not answers:\n",
    "        print(f\"❌ Error: No answers to evaluate!\")\n",
    "        return {}\n",
    "    \n",
    "\n",
    "    # Evaluate both answers for this question\n",
    "    evaluations = {}\n",
    "    llama_scores = []\n",
    "    llava_scores = []\n",
    "    file_output = open(output_file, \"w\")\n",
    "    \n",
    "    for i, (answer, prompt) in enumerate(zip(answers, rag_prompts)):\n",
    "        llama_similarity = evaluate_single_answer(prompt['llama_prompt'], answer['answer_llama'], model_name)\n",
    "        llava_similarity = evaluate_single_answer(prompt['llava_prompt'], answer['answer_llava'], model_name) if prompt['llava_prompt'] else 0.0\n",
    "        \n",
    "        evaluations[answer['question']] = {\n",
    "            'question': answer['question'],\n",
    "            'llama_answer': answer['answer_llama'],\n",
    "            'llava_answer': answer['answer_llava'],\n",
    "            'llama_similarity': llama_similarity,\n",
    "            'llava_similarity': llava_similarity\n",
    "        }\n",
    "        \n",
    "        llama_scores.append(llama_similarity)\n",
    "        llava_scores.append(llava_similarity)\n",
    "\n",
    "        print(f\"Question {i+1} Similarity Scores:\")\n",
    "        print(f\"  LLaMA: {llama_similarity:.4f}\")\n",
    "        print(f\"  LLaVA: {llava_similarity:.4f}\")\n",
    "        \n",
    "    # Calculate and display averages\n",
    "    llama_avg = sum(llama_scores) / len(llama_scores) if llama_scores else 0.0\n",
    "    llava_avg = sum(llava_scores) / len(llava_scores) if llava_scores else 0.0\n",
    "    \n",
    "    print(f\"\\n=== Evaluation Summary ===\")\n",
    "    print(f\"LLaMA Average Similarity: {llama_avg:.4f}\")\n",
    "    print(f\"LLaVA Average Similarity: {llava_avg:.4f}\")\n",
    "    \n",
    "    # Save results to file\n",
    "    save_similarity_results(evaluations, llama_avg, llava_avg, output_file)\n",
    "    \n",
    "    # Write best answers to answers.txt\n",
    "    try:\n",
    "        with open(\"answers.txt\", \"w\", encoding='utf-8') as f:\n",
    "            f.write(\"=== BEST ANSWERS BASED ON SIMILARITY SCORES ===\\n\\n\")\n",
    "            \n",
    "            for i, (question, evaluation_data) in enumerate(evaluations.items(), 1):\n",
    "                # Determine which model has higher similarity\n",
    "                if evaluation_data['llama_similarity'] > evaluation_data['llava_similarity']:\n",
    "                    best_model = \"LLaMA\"\n",
    "                    best_answer = evaluation_data['llama_answer']\n",
    "                    best_similarity = evaluation_data['llama_similarity']\n",
    "                else:\n",
    "                    best_model = \"LLaVA\"\n",
    "                    best_answer = evaluation_data['llava_answer']\n",
    "                    best_similarity = evaluation_data['llava_similarity']\n",
    "                \n",
    "                # Write to file\n",
    "                f.write(f\"Question {i}: {evaluation_data['question']}\\n\")\n",
    "                f.write(f\"Best Answer ({best_model} - Similarity: {best_similarity:.4f}):\\n\")\n",
    "                f.write(f\"{best_answer}\\n\\n\")\n",
    "                f.write(\"-\" * 80 + \"\\n\\n\")\n",
    "                \n",
    "                print(f\"Question {i}: {best_model} wins (Similarity: {best_similarity:.4f})\")\n",
    "        \n",
    "        print(f\"✅ Best answers written to answers.txt\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error writing best answers: {e}\")\n",
    "        \n",
    "    print(f\"✅ Evaluated {len(evaluations)} questions\")    \n",
    "    return evaluations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "0dacd9bf-8e6c-44b4-ad09-3be4cf5a3176",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\"\n",
    "    Main function to execute the RAG pipeline steps\n",
    "    \"\"\"\n",
    "    # TODO: add stoper for the entire process\n",
    "    # pdf switch\n",
    "    pdf_path = \"US11960514.pdf\"\n",
    "    \n",
    "    # Check if patent PDF exists\n",
    "    if not os.path.exists(pdf_path):\n",
    "        print(f\"Error: {pdf_path} not found in the current directory.\")\n",
    "        return\n",
    "    \n",
    "    print(\"=== RAG Pipeline for Patent Analysis ===\")\n",
    "    print(f\"Processing: {pdf_path}\\n\")\n",
    "    \n",
    "    # === STEP 1: CHUNKING ===\n",
    "    print(\"=== Step 1: Chunking the Patent ===\")\n",
    "    \n",
    "    # Check if we already have processed any chunks\n",
    "    all_metadata = load_chunks_metadata()\n",
    "    if pdf_path in all_metadata:\n",
    "        print(f\"Loaded existing chunks for {pdf_path}\")\n",
    "        chunks = all_metadata[pdf_path][\"chunks\"]\n",
    "    else:\n",
    "        print(\"Processing patent PDF...\")\n",
    "        all_metadata = extract_text_and_images_from_patent(pdf_path)\n",
    "        save_chunks_metadata(all_metadata)\n",
    "        chunks = all_metadata[pdf_path][\"chunks\"]\n",
    "    \n",
    "    # Print Step 1 summary\n",
    "    text_chunks = [c for c in chunks if c['type'] == 'text']\n",
    "    image_chunks = [c for c in chunks if c['type'] == 'image_description']\n",
    "    \n",
    "    print(f\"\\n=== Step 1 Summary ===\")\n",
    "    print(f\"Total chunks: {len(chunks)}\")\n",
    "    print(f\"Text chunks: {len(text_chunks)}\")\n",
    "    print(f\"Image chunks: {len(image_chunks)}\")\n",
    "    \n",
    "    # === STEP 2: VECTOR STORE ===\n",
    "    client, model = create_vector_store(chunks)\n",
    "    \n",
    "    # === STEP 3: QUESTION INPUT ===\n",
    "    questions = load_questions()\n",
    "    \n",
    "    # === STEP 4: RAG PROMPT CONSTRUCTION ===\n",
    "    if questions:  # Only proceed if we have questions\n",
    "        rag_prompts = process_questions_with_rag(questions, chunks, client, model)\n",
    "    else:\n",
    "        print(\"⚠️  No questions to process - skipping RAG prompt construction\")\n",
    "        rag_prompts = []\n",
    "    \n",
    "    # === STEP 5: ANSWER GENERATION ===\n",
    "    answers = []\n",
    "    if rag_prompts:  # Only proceed if we have prompts\n",
    "        answers = generate_answers(rag_prompts)\n",
    "    else:\n",
    "        print(\"⚠️  No prompts to process - skipping answer generation\")\n",
    "    \n",
    "    print(f\"\\n=== Pipeline Complete ===\")\n",
    "    print(f\"✅ Step 1: Patent chunked into {len(chunks)} pieces\")\n",
    "    print(f\"✅ Step 2: {len(text_chunks)} text chunks vectorized and stored\")\n",
    "    print(f\"✅ Step 3: {len(questions)} questions loaded and ready\")\n",
    "    print(f\"✅ Step 4: {len(rag_prompts)} RAG prompts constructed\")\n",
    "    print(f\"✅ Step 5: {len(answers)} answers generated and saved\")\n",
    "    \n",
    "    # Optional: Run evaluation if answers were generated\n",
    "    if answers and rag_prompts:\n",
    "        print(f\"\\n=== Optional: Running Answer Evaluation ===\")\n",
    "        evaluation_results = answers_eval(rag_prompts, answers)\n",
    "        return chunks, client, model, questions, rag_prompts, answers, evaluation_results\n",
    "    \n",
    "    return chunks, client, model, questions, rag_prompts, answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b8accc0-079d-4ab5-b24e-6b0626c4f8bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f5f3fc9-ac3f-4964-8582-5a558559b255",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== RAG Pipeline for Patent Analysis ===\n",
      "Processing: US11960514.pdf\n",
      "\n",
      "=== Step 1: Chunking the Patent ===\n",
      "Creating new metadata file: all_metadata.json\n",
      "Processing patent PDF...\n",
      "Processing 26 pages...\n",
      "📄 Processing page 1/26... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Neither CUDA nor MPS are available - defaulting to CPU. Note: This module is much faster with a GPU.\n",
      "c:\\Users\\amit5\\Desktop\\GenAI - Final project\\venv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "Neither CUDA nor MPS are available - defaulting to CPU. Note: This module is much faster with a GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting text chunk\n",
      "(✅ text)\n",
      "📄 Processing page 2/26... Converting image chunk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Neither CUDA nor MPS are available - defaulting to CPU. Note: This module is much faster with a GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(✅ image)\n",
      "📄 Processing page 3/26... Converting image chunk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Neither CUDA nor MPS are available - defaulting to CPU. Note: This module is much faster with a GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(✅ image)\n",
      "📄 Processing page 4/26... Converting image chunk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Neither CUDA nor MPS are available - defaulting to CPU. Note: This module is much faster with a GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(✅ image)\n",
      "📄 Processing page 5/26... Converting image chunk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Neither CUDA nor MPS are available - defaulting to CPU. Note: This module is much faster with a GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(✅ image)\n",
      "📄 Processing page 6/26... Converting image chunk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Neither CUDA nor MPS are available - defaulting to CPU. Note: This module is much faster with a GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(✅ image)\n",
      "📄 Processing page 7/26... Converting image chunk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Neither CUDA nor MPS are available - defaulting to CPU. Note: This module is much faster with a GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(✅ image)\n",
      "📄 Processing page 8/26... Converting image chunk\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50f4bf4f-9f9c-41fa-af94-8d5cd0fbc5fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae057207-0e98-4ce0-9286-95f719e3d005",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:percent"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
